{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "sUmTooTW3osf",
    "outputId": "b35d063e-611c-4d45-dbab-edf9f7d66b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct  3 14:13:51 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "P8wSlgXoDPCR",
    "outputId": "68a4f942-714d-4ebe-cd59-d1afa42e8336"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 278Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 62.6Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 664Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:15, 92.6Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 265Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 57.2Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 135Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"355M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "file_name = \"../input/LesMiserables.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Finetune GPT-2\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA IMPORTANT NOTE**: If running the notebook in Google Colab, it is advised to mount your Drive to be able to save checkpoints and re-start from where you stopped in case the notebook crashes or times out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "aeXshJM-Cuaf",
    "outputId": "fcfb2e28-1df4-4116-c326-1cdaf18734ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/355M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 1203667 tokens\n",
      "Training...\n",
      "[10 | 24.03] loss=3.25 avg=3.25\n",
      "[20 | 32.75] loss=3.31 avg=3.28\n",
      "[30 | 41.40] loss=3.33 avg=3.29\n",
      "[40 | 50.10] loss=3.19 avg=3.27\n",
      "[50 | 58.78] loss=3.05 avg=3.22\n",
      "[60 | 67.47] loss=3.07 avg=3.20\n",
      "[70 | 76.14] loss=3.48 avg=3.24\n",
      "[80 | 84.84] loss=3.25 avg=3.24\n",
      "[90 | 93.55] loss=3.51 avg=3.27\n",
      "[100 | 102.22] loss=3.31 avg=3.28\n",
      "[110 | 110.93] loss=3.07 avg=3.26\n",
      "[120 | 119.64] loss=2.94 avg=3.23\n",
      "[130 | 128.31] loss=2.94 avg=3.20\n",
      "[140 | 136.99] loss=2.96 avg=3.19\n",
      "[150 | 145.69] loss=3.07 avg=3.18\n",
      "[160 | 154.39] loss=2.88 avg=3.16\n",
      "[170 | 163.08] loss=2.89 avg=3.14\n",
      "[180 | 171.77] loss=3.12 avg=3.14\n",
      "[190 | 180.42] loss=2.81 avg=3.12\n",
      "[200 | 189.09] loss=2.97 avg=3.11\n",
      "[210 | 197.78] loss=3.13 avg=3.11\n",
      "[220 | 206.44] loss=3.12 avg=3.11\n",
      "[230 | 215.38] loss=2.80 avg=3.10\n",
      "[240 | 224.04] loss=3.07 avg=3.10\n",
      "[250 | 232.70] loss=2.83 avg=3.09\n",
      "[260 | 241.39] loss=3.20 avg=3.09\n",
      "[270 | 250.06] loss=2.86 avg=3.08\n",
      "[280 | 258.76] loss=3.12 avg=3.08\n",
      "[290 | 267.44] loss=2.40 avg=3.06\n",
      "[300 | 276.14] loss=2.78 avg=3.05\n",
      "[310 | 284.80] loss=3.33 avg=3.06\n",
      "[320 | 293.46] loss=2.63 avg=3.04\n",
      "[330 | 302.14] loss=2.80 avg=3.03\n",
      "[340 | 310.81] loss=2.80 avg=3.02\n",
      "[350 | 319.50] loss=2.98 avg=3.02\n",
      "[360 | 328.16] loss=3.11 avg=3.03\n",
      "[370 | 336.85] loss=2.61 avg=3.01\n",
      "[380 | 345.54] loss=2.50 avg=3.00\n",
      "[390 | 354.23] loss=2.64 avg=2.98\n",
      "[400 | 362.90] loss=2.07 avg=2.96\n",
      "[410 | 371.57] loss=2.58 avg=2.95\n",
      "[420 | 380.25] loss=2.80 avg=2.94\n",
      "[430 | 388.91] loss=2.68 avg=2.93\n",
      "[440 | 397.54] loss=2.66 avg=2.93\n",
      "[450 | 406.19] loss=3.11 avg=2.93\n",
      "[460 | 414.87] loss=2.84 avg=2.93\n",
      "[470 | 423.54] loss=2.62 avg=2.92\n",
      "[480 | 432.19] loss=3.16 avg=2.93\n",
      "[490 | 440.88] loss=2.82 avg=2.92\n",
      "[500 | 449.58] loss=2.58 avg=2.92\n",
      "======== SAMPLE 1 ========\n",
      " qui à son âme et qu’il se pensa de mourir dans cette débibliothèque, qui n’avait point bien vivant, n’ont moins moins pourquin que tout à mépaule. Il y avait ce moins bien sérieux, tout l’héritage.\n",
      "\n",
      "Quelle de toussaint, vives d’un mépaule. Un bon qui ne mangeait qu’à la mort, sans celle, celle qui dû bordait son âme. Il était un égoutier qui avait faîté, à quoi une personne mort.\n",
      "\n",
      "Pascal s’êtie de façon.\n",
      "\n",
      "Cet enfant se trouve, ce son enfant lui dêpait, et à la bijou dans son palais ; d’un son palais que d’un égoutier. Le vieux précipice et le mépaule qui a dépensé se disent que une étendu dont la mort à tard. Les génésiasques le se sont déménagerieuses ; une être vieille à cet aînée qui faisait ausser sa pensée. Que puis ? tous étaient toutes. On poussait dans ce faïence. Je vous ai doute dans sa façon. On ne peut qu’une voix et un ami qui faisait vécu le mépaule. On ne peut qu’une femme. D’un cinquième était pourtant que le palais de tête de je ne sais étreint l’habitude. Voilà ce que j’ai fait vénérable dans son palais. On ne parvint, que je la fait voir quelque chose sur ce dûr que cela vous fait. J’avais un peu gênement le même. J’ai vu la maison de l’égout. J’ai vu le mépaule. J’ai va-ta monsieur j’une aubanite, j’avais un chiffonnier dans son palais. J’ai va-ta chien. Mais commençon. Je vous voudrez, je vous voudrez, je vois si vous ai été mal. J’en présentrait le génération. J’en fâche qu’on voudrez ; je vous n’aurai me pu donc dans le génération ? Je la m’agoutrez. Il faut qu’il faisait sûr. Je la la veille, monsieur. Je le suis précis, je la marche ; il faut qu’il faisait sa dévotion, moi dans c’est un ami. Il fâche. I n’était rien dans son chemin de bonheur, le nom ! Mais monsieur, seulement dans une nuit le saper ! Mais monsieur, s’est dans un enfant, un oisé. Mais ma vivir ! la vivir ou la victriel ! Vous êtes plusieurs ! Un homme, il fait bonheur que sa jolie pour moins pour moins oublié, et il fait l’humour dans ses tères. Il faisait qu’on appelait d’une sombre sombre vivant, un mieux vieux, un palais vieux. J’autre chose tout au droit.\n",
      "\n",
      "J’étaisser un peu de sœur.\n",
      "\n",
      "A bien même, il y a l’heure d’une fille qui voudrait l’homme. Il y a l’heure d’une petite cépine. Un son vieux-ce mal à la fille tromper sous et sous, seul fier un d\n",
      "\n",
      "[510 | 485.68] loss=2.73 avg=2.91\n",
      "[520 | 494.36] loss=2.74 avg=2.91\n",
      "[530 | 503.05] loss=2.74 avg=2.90\n",
      "[540 | 511.92] loss=2.23 avg=2.89\n",
      "[550 | 520.62] loss=2.93 avg=2.89\n",
      "[560 | 529.30] loss=2.44 avg=2.88\n",
      "[570 | 537.99] loss=2.59 avg=2.87\n",
      "[580 | 546.70] loss=2.30 avg=2.86\n",
      "[590 | 555.35] loss=2.39 avg=2.85\n",
      "[600 | 564.01] loss=2.77 avg=2.85\n",
      "[610 | 572.69] loss=2.70 avg=2.84\n",
      "[620 | 581.38] loss=2.91 avg=2.84\n",
      "[630 | 590.06] loss=2.85 avg=2.84\n",
      "[640 | 598.71] loss=2.89 avg=2.84\n",
      "[650 | 607.36] loss=2.87 avg=2.85\n",
      "[660 | 616.03] loss=2.64 avg=2.84\n",
      "[670 | 624.69] loss=2.70 avg=2.84\n",
      "[680 | 633.38] loss=3.17 avg=2.85\n",
      "[690 | 642.06] loss=2.58 avg=2.84\n",
      "[700 | 650.73] loss=2.40 avg=2.83\n",
      "[710 | 659.39] loss=2.04 avg=2.82\n",
      "[720 | 668.05] loss=2.42 avg=2.81\n",
      "[730 | 676.71] loss=2.96 avg=2.81\n",
      "[740 | 685.39] loss=2.57 avg=2.81\n",
      "[750 | 694.06] loss=2.61 avg=2.80\n",
      "[760 | 702.71] loss=2.94 avg=2.81\n",
      "[770 | 711.38] loss=2.19 avg=2.79\n",
      "[780 | 720.04] loss=2.19 avg=2.78\n",
      "[790 | 728.71] loss=2.65 avg=2.78\n",
      "[800 | 737.36] loss=2.23 avg=2.77\n",
      "[810 | 746.04] loss=2.74 avg=2.77\n",
      "[820 | 754.73] loss=3.02 avg=2.77\n",
      "[830 | 763.43] loss=2.85 avg=2.78\n",
      "[840 | 772.13] loss=3.26 avg=2.78\n",
      "[850 | 780.80] loss=2.85 avg=2.79\n",
      "[860 | 789.46] loss=2.85 avg=2.79\n",
      "[870 | 798.15] loss=2.67 avg=2.78\n",
      "[880 | 806.84] loss=2.30 avg=2.78\n",
      "[890 | 815.65] loss=2.59 avg=2.77\n",
      "[900 | 824.37] loss=2.14 avg=2.76\n",
      "[910 | 833.04] loss=2.21 avg=2.75\n",
      "[920 | 841.71] loss=2.26 avg=2.74\n",
      "[930 | 850.39] loss=2.53 avg=2.74\n",
      "[940 | 859.08] loss=2.53 avg=2.74\n",
      "[950 | 867.76] loss=2.69 avg=2.74\n",
      "[960 | 876.43] loss=2.74 avg=2.74\n",
      "[970 | 885.09] loss=2.39 avg=2.73\n",
      "[980 | 893.72] loss=2.37 avg=2.73\n",
      "[990 | 902.38] loss=2.39 avg=2.72\n",
      "[1000 | 911.05] loss=2.70 avg=2.72\n",
      "Saving checkpoint/run1/model-1000\n",
      "======== SAMPLE 1 ========\n",
      "ez au jardin, n’avaient toujours une chambre. Dans l’infamie, cet anciemest du bonheur péripétait plus grande à la terre, mais ils entendent et semblent restés. Il avait disparu les écoutables.\n",
      "\n",
      "– Oh ! c’est que tu te reas bien ? reprit l’enfant.\n",
      "\n",
      "Dans le tassement de nuit, de l’infamie, quelque jour, il était évidemment écrasé. Quelle chère chère ? le grand jardin. Le jardin était peu de temps de l’âme.\n",
      "\n",
      "– Tiens ! dit la lune.\n",
      "\n",
      "Dans le tassement toussaint la lune se sentit qu’ils étaient éternelles. Il n’avait que ma sœur, ma sœur, ce qu’il avait deviné faire peut-être dire une sourire. Il n’avait pas résulté ainsi, il répondait à l’une en effet :\n",
      "\n",
      "– Quelquefois !\n",
      "\n",
      "Dans sa chambre il se sentait dire que l’un est de l’autre.\n",
      "\n",
      "La vieille sœur lui vient de cette fois une sorte de sang qu’elle vient. C’est en effet que la sourire fut bien affreuse.\n",
      "\n",
      "Puis il entendait ajouté, car, comme la maison, cet inconnu, il n’y avait plus les événements, il n’y voyait pas. Ce matin il était resté à l’églé, et une petite mort grosse se dégageait en ruelle ou en ruelle si vâté là. Cette ruelle étant l’hiver, et l’une appelait le bien.\n",
      "\n",
      "Des règlements, des généraux, d’en bas, d’abord, de résultats, d’un homme ; des échafaudages, d’en bas, d’abord, d’un mouchoir ; des familles, d’en bas, d’abord, de résultats.\n",
      "\n",
      "Il l’entendit de la fin.\n",
      "\n",
      "Un jour mots s’adressent.\n",
      "\n",
      "– C’est dégageé, dit l’enfant.\n",
      "\n",
      "Il ajouta avec une forme blanche qui s’appellait enthousiaste en haut de l’ombre :\n",
      "\n",
      "– Oui, monsieur !\n",
      "\n",
      "Et il entendit une voix d’enfant qui ne se dégageait pas éteint :\n",
      "\n",
      "– Mon père,\n",
      "\n",
      "– Mon père !\n",
      "\n",
      "Et il dit avec une accent profonde qui lui avait toussaint :\n",
      "\n",
      "– Oh ! dit l’enfant, si s’il y a du père, c’est d’arriver des étoiles dans le ventre.\n",
      "\n",
      "La sourire n’était pauvre. En jouant le matin il recommença à ces voix-là :\n",
      "\n",
      "– C’est d’en bas dans l’obscur d’un mot.\n",
      "\n",
      "Il lui sembla qu’il était trop sortant dans l’église, de ne puis pour entrer cette obscurité, mais il recommença à l’invisible qu’elle ne lui semble point à ce mot.\n",
      "\n",
      "Le monde qui les nous venait de l’argot lui avait eu des mains de l’éblouissement, dans toussaint, dans l’infamy d’abord, et dans sa fange, d’affaires. Toussaint, cette femme, ne lui paraissait plus, n’emprincent si l’oreille de vous, à la fange, et ce petit jaloux qui ne\n",
      "\n",
      "[1010 | 950.78] loss=2.53 avg=2.72\n",
      "[1020 | 959.46] loss=3.02 avg=2.72\n",
      "[1030 | 968.15] loss=2.62 avg=2.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1040 | 976.85] loss=2.06 avg=2.71\n",
      "[1050 | 985.50] loss=2.64 avg=2.71\n",
      "[1060 | 994.14] loss=3.15 avg=2.72\n",
      "[1070 | 1002.85] loss=2.65 avg=2.71\n",
      "[1080 | 1011.51] loss=2.66 avg=2.71\n",
      "[1090 | 1020.18] loss=2.76 avg=2.71\n",
      "[1100 | 1028.88] loss=2.80 avg=2.72\n",
      "[1110 | 1037.57] loss=1.84 avg=2.70\n",
      "[1120 | 1046.25] loss=2.09 avg=2.69\n",
      "[1130 | 1054.96] loss=2.63 avg=2.69\n",
      "[1140 | 1063.63] loss=2.24 avg=2.69\n",
      "[1150 | 1072.28] loss=2.78 avg=2.69\n",
      "[1160 | 1080.96] loss=2.42 avg=2.68\n",
      "[1170 | 1089.64] loss=2.70 avg=2.68\n",
      "[1180 | 1098.31] loss=2.98 avg=2.69\n",
      "[1190 | 1106.95] loss=2.61 avg=2.69\n",
      "[1200 | 1115.63] loss=2.45 avg=2.68\n",
      "[1210 | 1124.46] loss=2.70 avg=2.68\n",
      "[1220 | 1133.12] loss=2.44 avg=2.68\n",
      "[1230 | 1141.80] loss=2.15 avg=2.67\n",
      "[1240 | 1150.45] loss=2.87 avg=2.68\n",
      "[1250 | 1159.16] loss=2.65 avg=2.68\n",
      "[1260 | 1167.86] loss=2.70 avg=2.68\n",
      "[1270 | 1176.54] loss=2.72 avg=2.68\n",
      "[1280 | 1185.20] loss=2.47 avg=2.67\n",
      "[1290 | 1193.90] loss=2.85 avg=2.68\n",
      "[1300 | 1202.58] loss=2.88 avg=2.68\n",
      "[1310 | 1211.22] loss=2.54 avg=2.68\n",
      "[1320 | 1219.87] loss=2.45 avg=2.67\n",
      "[1330 | 1228.54] loss=2.78 avg=2.67\n",
      "[1340 | 1237.22] loss=2.14 avg=2.67\n",
      "[1350 | 1245.88] loss=2.14 avg=2.66\n",
      "[1360 | 1254.55] loss=2.79 avg=2.66\n",
      "[1370 | 1263.24] loss=2.16 avg=2.66\n",
      "[1380 | 1271.90] loss=2.43 avg=2.65\n",
      "[1390 | 1280.58] loss=2.20 avg=2.65\n",
      "[1400 | 1289.27] loss=2.53 avg=2.65\n",
      "[1410 | 1297.96] loss=2.32 avg=2.64\n",
      "[1420 | 1306.61] loss=2.22 avg=2.64\n",
      "[1430 | 1315.26] loss=2.30 avg=2.63\n",
      "[1440 | 1323.93] loss=2.70 avg=2.63\n",
      "[1450 | 1332.60] loss=2.44 avg=2.63\n",
      "[1460 | 1341.26] loss=2.49 avg=2.63\n",
      "[1470 | 1349.93] loss=2.95 avg=2.63\n",
      "[1480 | 1358.64] loss=2.52 avg=2.63\n",
      "[1490 | 1367.32] loss=2.29 avg=2.63\n",
      "[1500 | 1376.00] loss=2.32 avg=2.62\n",
      "======== SAMPLE 1 ========\n",
      " passage même, ce qu’il s’était au. Il n’était pas pour aller miroire qu’on eût eu mieux vraiment mal de ne pas rien, par exemple, de dire que venez, j’ai pris ces paroles. Que faire ? que m’était-il très loin ? parfois que ce sont les paupières ? que la voile était très effrayante ? que j’avais à moitié jeune, et que j’avais trouvé que, quoiqu’ici donc, je suis vrai de manger des choses qu’il avait fait qu’il faisait ? Par dans l’entrée et en ménageant toutes sortes qu’il venait d’elle, je se disrait dans une haute petite fille, que ce n’était pas là cela jusqu’à son nom. O mon Dieu ! vous ne parlez pas que vous êtes vrai ! Venez-moi le dire : C’est mon père ! je les sais. Vous avez des choses qu’elle ne s’agit pas deux ; êtes-vous là ? Ah ! c’était une fille du bon Dieu, c’était qu’elle n’étonlait point. Elle n’a pas deux, dorment un mot. Elle a sa fille comme c’était un dallage de sa femme, c’était une femme de Chénéal. Cela a bien fait pour toutes ces deux figures, c’était une sorte de bonne heure. L’on dit qu’elle est aisée de votre façon, le bon Dieu dit tout à coup : C’est vrai, vrai ! il a écrit : C’est malade, c’est malade ! cela est une mauvaise femme, cela est un sienne et tricot, cela est un petit nom, c’est un petit garçon ou trois par les frère ; c’est un bon pauvre vieux pauvre ; c’est un chaud, c’est un peu ! tout ce faisant, vous me la réponds, je n’ai qu’à laisse avec ma bonne femme ; pas m’amuse, je n’ai qu’à lire sans m’amuse. Mon bienve, moi m’en ai, que me dire, elle s’en arrêt, je n’ai qu’à laisse avec ma bonne fame, elle a donc qu’à laisser un soir qu’il y faisait, j’aurais là à m’eux un mot comme vrai, j’aurais le sœur d’amour, ou, je n’ai qu’à m’adresse, elle était morte. Cela se dressent, ce fut une vraie révolution. Je ne suis plus de cette révolution, mais ce n’était pas un mauvais écrivain. M’a cria : Oh ! qu’est-ce que cela soit une faute ? Votre ma pauvre ! qu’est-ce que ceux-là sont la nuit ? M’a tiré tout cela. C’est que ce n’était pas vrai au milieu de cette heure du mardi gras, au moins du peuple, au peuple de mémoire et au mémoire de music, même de cette nuit de la lune parce que l’aise poussent, qu’on pourrait garder comme à peu prêter, c’est que cela veut dire tout à coup, qui fait votre faute, c’est que vous n’avez pas cela sur de ses deux émotions, ce qui a\n",
      "\n",
      "[1510 | 1408.55] loss=2.26 avg=2.62\n",
      "[1520 | 1417.25] loss=1.90 avg=2.61\n",
      "[1530 | 1426.08] loss=2.85 avg=2.61\n",
      "[1540 | 1434.78] loss=2.36 avg=2.61\n",
      "[1550 | 1443.43] loss=2.67 avg=2.61\n",
      "[1560 | 1452.10] loss=2.68 avg=2.61\n",
      "[1570 | 1460.79] loss=2.79 avg=2.61\n",
      "[1580 | 1469.46] loss=2.76 avg=2.61\n",
      "[1590 | 1478.17] loss=2.32 avg=2.61\n",
      "[1600 | 1486.86] loss=1.86 avg=2.60\n",
      "[1610 | 1495.53] loss=2.48 avg=2.60\n",
      "[1620 | 1504.21] loss=2.22 avg=2.59\n",
      "[1630 | 1512.87] loss=2.53 avg=2.59\n",
      "[1640 | 1521.57] loss=2.77 avg=2.60\n",
      "[1650 | 1530.27] loss=2.15 avg=2.59\n",
      "[1660 | 1538.93] loss=2.67 avg=2.59\n",
      "[1670 | 1547.58] loss=2.22 avg=2.59\n",
      "[1680 | 1556.24] loss=1.68 avg=2.58\n",
      "[1690 | 1564.92] loss=1.78 avg=2.57\n",
      "[1700 | 1573.60] loss=2.07 avg=2.56\n",
      "[1710 | 1582.27] loss=1.67 avg=2.55\n",
      "[1720 | 1590.97] loss=1.80 avg=2.54\n",
      "[1730 | 1599.66] loss=2.04 avg=2.53\n",
      "[1740 | 1608.34] loss=2.13 avg=2.53\n",
      "[1750 | 1617.01] loss=2.27 avg=2.53\n",
      "[1760 | 1625.66] loss=1.95 avg=2.52\n",
      "[1770 | 1634.32] loss=2.40 avg=2.52\n",
      "[1780 | 1642.99] loss=2.40 avg=2.52\n",
      "[1790 | 1651.66] loss=2.50 avg=2.52\n",
      "[1800 | 1660.33] loss=2.37 avg=2.51\n",
      "[1810 | 1668.99] loss=2.58 avg=2.52\n",
      "[1820 | 1677.66] loss=2.47 avg=2.51\n",
      "[1830 | 1686.33] loss=2.05 avg=2.51\n",
      "[1840 | 1695.03] loss=2.77 avg=2.51\n",
      "[1850 | 1703.73] loss=1.72 avg=2.50\n",
      "[1860 | 1712.40] loss=2.54 avg=2.50\n",
      "[1870 | 1721.07] loss=2.69 avg=2.51\n",
      "[1880 | 1729.93] loss=2.03 avg=2.50\n",
      "[1890 | 1738.61] loss=1.62 avg=2.49\n",
      "[1900 | 1747.25] loss=2.11 avg=2.48\n",
      "[1910 | 1755.94] loss=2.06 avg=2.48\n",
      "[1920 | 1764.62] loss=2.32 avg=2.48\n",
      "[1930 | 1773.30] loss=2.20 avg=2.47\n",
      "[1940 | 1781.98] loss=1.68 avg=2.47\n",
      "[1950 | 1790.68] loss=2.54 avg=2.47\n",
      "[1960 | 1799.33] loss=2.63 avg=2.47\n",
      "[1970 | 1808.00] loss=2.52 avg=2.47\n",
      "[1980 | 1816.68] loss=2.01 avg=2.46\n",
      "[1990 | 1825.36] loss=2.10 avg=2.46\n",
      "[2000 | 1834.06] loss=2.11 avg=2.46\n",
      "Saving checkpoint/run1/model-2000\n",
      "CPU times: user 21min 32s, sys: 6min 46s, total: 28min 19s\n",
      "Wall time: 31min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='355M',\n",
    "              steps=2000,\n",
    "              restore_from='fresh', #change to 'latest' if restarting fine-tuning from saved checkpoint\n",
    "              checkpoint_dir = \"drive/My Drive/gpt2/checkpoint\", #if wanting to save checkpoints on Drive\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              learning_rate=1e-4,\n",
    "              sample_every=500, \n",
    "              save_every=1000, #if the fine-tuning is very slow, lower this number \n",
    "              overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.reset_session(sess)\n",
    "del sess\n",
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "-fxL77nvAMAX",
    "outputId": "d77b8f3d-9cd8-4d7a-bd66-39a42ca284ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-2000\n",
      "CPU times: user 10.5 s, sys: 512 ms, total: 11 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "## Generate Text From The Trained Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4RNY6RBI9LmL",
    "outputId": "acd92914-6e51-4436-af84-2380919f1741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jusqu’à ce que je sois fait, il ne le comprend pas.\n",
      "\n",
      "Il était dit, parmi les gens qui les avait réfractées, que je peux. J’en sais qu’il le soir, et que je peux.\n",
      "\n",
      "Le soir est une sorte de joie que vous devez toujours échappées. Il se réveille, et il vous repose.\n",
      "\n",
      "Le soir, je t’ai vu l’élément qui passe.\n",
      "\n",
      "C’est que je peux, il n’y a rien d’étourdissement.\n",
      "\n",
      "Le soir, je le sais.\n",
      "\n",
      "Sur ce pauvre chose, j’ai pris l’épanouissement.\n",
      "\n",
      "J’en suis, je vous laisse.\n",
      "\n",
      "Le soir, je le sais.\n",
      "\n",
      "Quelqu’un qui veut attendre, c’est que je peux.\n",
      "\n",
      "Cela, c’est, pour le couvent, que je n’en veux pas ici. Il est mort.\n",
      "\n",
      "Quand il est mort, je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "J’ai pourtant qu’on peut être mort d’être le moment où je ne vois pas sortir de cet homme pour sortir de la vie.\n",
      "\n",
      "Je n’en veux pas.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Vous me faites l’ignominie.\n",
      "\n",
      "J’en sais, j’ai fait une figure.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "J’en avois de la saison.\n",
      "\n",
      "Je ne sais pas.\n",
      "\n",
      "J’en avais bien un regard.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "J’en avais bien un regard.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "J’en avais bien un regard.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "J’en avais besoin de ce couvent.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "Je le sais.\n",
      "\n",
      "\n",
      "\n",
      "– Monsieur, dit le femme, dans une chambre où vous voyez, je le sais.\n",
      "\n",
      "– Non, dit-il, j’ai donc vous avez eu des gens qui faillites, vous le mettrez ; et je laisse.\n",
      "\n",
      "– Enfin, dit le femme.\n",
      "\n",
      "– Vous avez ainsi toujours que vous vous parlez.\n",
      "\n",
      "– Parlez ! pit le nom de la femme.\n",
      "\n",
      "– Non, dit-il, j’en ai fait un peu de la fois des jeunes filles, et je n’en veux pas.\n",
      "\n",
      "– Parlez ! est-ce que je ne vais pas encore vous être une fille ?\n",
      "\n",
      "– Je n’en veux pas que je suis, et je ne laisse pas.\n",
      "\n",
      "– Parlez, dit le femme.\n",
      "\n",
      "– Je ne sais pas.\n",
      "\n",
      "– Des femmes, dit le femme.\n",
      "\n",
      "– J’en\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text = gpt2.generate(sess, return_as_list=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8DKMc0fiej4N",
    "outputId": "526dbd1b-f076-4f47-a42b-058ab4a35b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Monsieur les deux autres, et dans cet homme, l’aspiration de l’enfant, le bonheur de la mère, vous, monsieur, vous, je n’ai pas besoin de la lumière pour vous, monsieur, je n’ai pas besoin de la lumière pour vous. Je vous le dire que je ne verrai que de vous, et que je vous connaît. Je vous ai vécu les ritements qui sont difformes, les soixante jouissements, les ténèbres, les gonds de l’enfant, les choses de la vie, les mères du moi, les étoiles et les couverts de la vie, le soir, le voile de l’enfant, et les choses de l’aspiration, le bonheur de la mère.\n",
      "\n",
      "Il poursuivit : – Je vous m’appelle, en ce cas. Je vous ai donc à vous. Je vous ai dit à vous. Il y a des jouissances toujours où je pense que je vous ai dit. Il y a des hommes qui sont des hommes, et je vous ai réunissé. C’est des gens qui ont les âmes de la vie. J’ai de la voir une voiture de jouissances, parce que vous alliez de la vie, c’est un grand trot jouissant et sévère.\n",
      "\n",
      "C’était une sorte d’arrête de lui qui le voyait bienveillant.\n",
      "\n",
      "Quand le maire vivait, elle leva son front et regarda son regard sur l’enfant qui lui poussa dans sa jeunesse.\n",
      "\n",
      "Elle ne put se couvert de rire, et elle se sentait que j’ai pu voir sa jeunesse.\n",
      "\n",
      "Il y eut une pause.\n",
      "\n",
      "– Monsieur,\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=500,\n",
    "              temperature=0.7,\n",
    "              top_k=50,\n",
    "              prefix=\"- Monsieur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4G\tcheckpoint/run1\r\n",
      "1.4G\tcheckpoint/\r\n"
     ]
    }
   ],
   "source": [
    "!du -h checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      length=500,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-LRex8lfv1g"
   },
   "outputs": [],
   "source": [
    "# may have to run twice to get file to download\n",
    "files.download(gen_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint/run1/\n",
      "checkpoint/run1/model-2000.meta\n",
      "checkpoint/run1/model-2000.data-00000-of-00001\n",
      "checkpoint/run1/counter\n",
      "checkpoint/run1/encoder.json\n",
      "checkpoint/run1/vocab.bpe\n",
      "checkpoint/run1/checkpoint\n",
      "checkpoint/run1/hparams.json\n",
      "checkpoint/run1/model-2000.index\n",
      "checkpoint/run1/events.out.tfevents.1570112199.b225f5908fdf\n"
     ]
    }
   ],
   "source": [
    "!tar czvf ./model-lesmiserables.tar.gz checkpoint/run1/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb  model-lesmiserables.tar.gz  samples\r\n",
      "checkpoint\t\t   models\r\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"./model-lesmiserables.tar.gz\">Download File</a>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train a GPT-2 Text-Generating Model w/ GPU",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
